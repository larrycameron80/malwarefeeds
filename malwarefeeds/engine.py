# -*- coding: utf-8 -*-
"""An Engine to handle the malware feeds."""

import logging
import re
import urllib

import feedparser


class Engine(object):
    """An Engine to handle the malware feeds."""

    def __init__(self):
        """Initialize the Engine object."""
        self.malcode = None
        self.mdl = None
        self.vxvault = None
        self.malwareurls = None
        self.ransomware = None
        self._user_agent = {'User-agent': 'Mozilla/5.0 (X11; U; Linux i686)'}

    def update(self):
        """Fetch the content from all feeds."""
        self.malcode = self._get_malcode()
        self.mdl = self._get_mdl()
        self.vxvault = self._get_vxvault()
        self.malwareurls = self._get_malwareurls()
        self.ransomware = self._get_ransomware()

    def read(self, fn_callback=None):
        """
        Read and parse all feeds.

        :param fn_callback: A callback function to retrieve feed name and URL
        """

        if self.malcode is not None:
            for url in self._parse_xml_list_desc(self.malcode):
                if fn_callback:
                    fn_callback(u'Malc0de', url)
                else:
                    yield u'Malc0de', url

        if self.mdl is not None:
            for url in self._parse_xml_list_desc(self.mdl):
                if fn_callback:
                    fn_callback(u'Malware Domain List', url)
                else:
                    yield u'Malware Domain List', url

        if self.vxvault is not None:
            for url in self._parse_txt_file(self.vxvault):
                if fn_callback:
                    fn_callback(u'VxVault', url)
                else:
                    yield u'VxVault', url

        if self.malwareurls is not None:
            for url in self._parse_txt_file(self.malwareurls, b'\n'):
                if fn_callback:
                    fn_callback(u'Malware URLs', url)
                else:
                    yield u'Malware URLs', url

        if self.ransomware is not None:
            for url in self._parse_txt_file(self.ransomware, b'\n'):
                if fn_callback:
                    fn_callback(u'Ransomware Tracker', url)
                else:
                    yield u'Ransomware Tracker', url

        return True

    def _download(self, url):
        """
        Execute download from a URL.

        :param url: URL to access
        :return: urllib response or None
        """
        try:
            req = urllib.request.Request(url, headers=self._user_agent)
            with urllib.request.urlopen(req) as response:
                return response.read()
        except Exception as e:
            logging.error(u'Request: %s Failure: %s' % (url, e))
            return None

    def _get_malcode(self):
        """
        Get URL http://malc0de.com/rss content.

        :return: The Malc0de URL content
        """
        return self._download(u'http://malc0de.com/rss')

    def _get_mdl(self):
        """
        Get URL http://www.malwaredomainlist.com/hostslist/mdl.xml content.

        :return: The Malware Domain List URL content
        """
        return self._download(u'http://www.malwaredomainlist.com/hostslist/mdl.xml')

    def _get_vxvault(self):
        """
        Get URL http://vxvault.net/URL_List.php content.

        :return: The VxVault URL content
        """
        return self._download(u'http://vxvault.net/URL_List.php')

    def _get_malwareurls(self):
        """
        Get URL http://malwareurls.joxeankoret.com/normal.txt content.

        :return: The Joxean Koret Malware URLs content
        """
        return self._download(u'http://malwareurls.joxeankoret.com/normal.txt')

    def _get_ransomware(self):
        """
        Get URL https://ransomwaretracker.abuse.ch/downloads/RW_URLBL.txt content.

        :return: The Ransomware Tracker URLs content
        """
        return self._download(u'https://ransomwaretracker.abuse.ch/downloads/RW_URLBL.txt')

    def _parse_xml_list_desc(self, response):
        """Parse a XML Feed."""
        feed = feedparser.parse(response)

        for entry in feed.entries:
            desc = entry.description.split(' ')
            url = desc[1].rstrip(',')

            if url == '':
                continue

            if url == '-' and len(desc) >= 5:
                url = desc[4].rstrip(',')
            # troca &amp; por &
            url = re.sub('&amp;', '&', url)
            # senao tem http no inicio, coloca
            if not re.match('http', url):
                url = 'http://' + url
            yield url

    def _parse_txt_file(self, response, splitter=b'\r\n'):
        """Parse text files and split it."""
        if response:
            for url in response.split(splitter):
                if url.startswith(b'http'):
                    yield url.decode()
